import numpy as np  #numpy ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš© (ì´ë¦„ì€ npë¡œ)
import matplotlib.pyplot as plt #matplotlib.pyplot ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš© (ì´ë¦„ì€ pltë¡œ)
import pandas as pd #pandas ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš© (ì´ë¦„ì€ pdë¡œ)
import math# ì˜¬ë¦¼ì„ êµ¬í•˜ê¸° ìœ„í•´ math ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©
from matplotlib.lines import Line2D

#data class0: ì²­ì‚¬ê³¼, 1:ì‚¬ê³¼, 2:ë°”ë‚˜ë‚˜, 3:ë¸”ë™ë² ë¦¬, 4:ì˜¤ì´, 5:ì˜¤ë Œì§€, 6:ë³µ.ìˆ­ì•„, 7:ë°° 
# class8:í† ë§ˆí† , 9:ìˆ˜ë°•

#C:\\Users\\ì´ìƒí›ˆ\\OneDrive - í•œêµ­ê³µí•™ëŒ€í•™êµ\\ë°”íƒ• í™”ë©´\\ì¸ì„¤\\data_set
#data_y ìƒì„±



#ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜
def logistic(z):
    
    logi=1/(1+np.exp(-z))
    
    return logi

#one-hot encodingí•¨ìˆ˜
def One_Hot_Encoding(data_y):
    N=len(data_y)#ë°ì´í„°ì˜ ì´ ê°œìˆ˜
    Q_class=np.unique(data_y)#í´ë˜ìŠ¤ ì¢…ë¥˜
    Q=len(Q_class)#í´ë˜ìŠ¤ ê°œìˆ˜
    
    #one-hot-yì´ˆê¸°í™”
    one_hot_y=np.zeros([N,Q])
    #ê° í´ë˜ìŠ¤ì— ëŒ€í•´ One-Hot í‘œí˜„ìœ¼ë¡œ ë³€í™˜
    for k in np.arange(0,N,1):
        #í•´ë‹¹ ë¹„êµê°’ì´ ìë™ìœ¼ë¡œ floatí˜•ìœ¼ë¡œ ë³€í™˜ ë˜ì–´ ì €ì¥ë¨(ë„˜íŒŒì´ ë°°ì—´ íŠ¹ì§•)
        one_hot_y[k,:]=(data_y[k]==Q_class)
        
    return one_hot_y



#ì˜¤ì°¨ ì—­ì „íŒŒì•Œê³ ë¦¬ì¦˜(batch size ì¡°ì ˆê°€ëŠ¥)
def Error_Back_Propagation1(Train,Node,batch,epoch):
    
    #ê¸°ë³¸ ì„¤ê³„ë¥¼ ìœ„í•œ ë°ì´í„°ì…‹ ë¶„í• 
  
    data=Train
    #inputë°ì´í„° ì†ì„± ìˆ˜
    input_N=data.shape[1]-1
    data_x=data[:,:input_N]
    data_y=data[:,input_N:input_N+1]
    
    #ë°ì´í„° ì´ ê°œìˆ˜
    N=len(data_x)
    one=np.ones([len(data_x),1])
    one1=np.ones([1,1])
   
    #Output í´ë˜ìŠ¤ ìˆ˜
    Q=len(np.unique(data_y))
    #íˆë“  layer ë…¸ë“œ ìˆ˜ 
    L=Node
    #ì…ë ¥ layerì™€ íˆë“  layer ì‚¬ì´ì˜ weight ì´ˆê¸°í™”
    v=(np.random.rand(L,input_N+1)*2-1)
    #íˆë“  layerì™€ ì¶œë ¥ layer ì‚¬ì´ì˜ weight ì´ˆê¸°í™”
    w=(np.random.rand(Q,L+1)*2-1)
    
    learningrate=0.005#0.1:í•™ìŠµ ì‹¤íŒ¨ ,0.05:í•™ìŠµì‹¤íŒ¨,0.01:ì •í™•ë„0.7ì •ë„(ë‚®ì€í¸ì¸ê±°ê°™ìŒ)
    acc=[]
    mse=[]
    w_all=[]
    v_all=[]
    
    #ì—í­
    for i in np.arange(0,epoch,1):
        #ë°ì´í„°ì…‹ ì…”í”Œ+ë¶„í• 
        np.random.shuffle(data)
        data_x=data[:,:input_N]
        data_y=data[:,input_N:input_N+1]
        
        #yê°’ one-hotí˜•íƒœë¡œ ë³€í™˜
        one_hot_y=One_Hot_Encoding(data_y)
        #ë”ë¯¸ë…¸ë“œë¥¼ ì¶”ê°€í•œ xë°ì´í„° ìƒì„±
        x=np.concatenate((data_x,one),axis=1)
        
        #ë°ì´í„° ì—…ë°ì´íŠ¸
  
        for n in np.arange(0,N,batch):         
            #ë°°ì¹˜ ì‚¬ì´ì¦ˆì™€ ë°ì´í„°ì…‹ì´ ë‚˜ëˆ„ì–´ ë–¨ì–´ì§€ì§€ ì•Šì„ ë•Œë¥¼ ìœ„í•´ êµ¬í•¨
            maxN = min(batch, N - n) 
            one1=np.ones([1,maxN])
            #íˆë“  layerì˜ aê°’ì„ êµ¬í•¨
            a=(v@x[n:n+maxN,:].T).reshape(L,maxN)
            #aê°’ì„ ë¡œì§€ìŠ¤í‹± í•¨ìˆ˜ì— í†µê³¼ì‹œì¼œ bë¥¼ êµ¬í•¨
            b=logistic(a)
            b=np.concatenate((b,one1),axis=0)
            #ì¶œë ¥  layerì˜ Bê°’ì„ êµ¬í•¨
            B=w@b
            #Bê°’ì„ ë¡œì§€ìŠ¤í‹± í•¨ìˆ˜ì— í†µê³¼ì‹œì¼œ y_hatì„ êµ¬í•¨
            y_hat=logistic(B)
            y_hat=y_hat.T
            #gradientê°’ì„ ì €ì¥í•  ë„˜íŒŒì´ ë°°ì—´ ìƒì„±
            learnv=np.zeros((L,input_N + 1))
            learnw=np.zeros((Q,L+1))
            #ë¸íƒ€ ê°’ ê³„ì‚°
            delta = 2*(y_hat-one_hot_y[n:n+maxN])*y_hat*(1-y_hat)
            delta=delta.T      
            #vì—…ë°ì´íŠ¸ë¥¼ ìœ„í•œ b(1-b)ê³„ì‚°
            b_v=b[:len(v),:].reshape(len(v),maxN)*(1-b[:len(v),:].reshape(len(v),maxN))
            #gradient ê°’ ê³„ì‚°
            learnv=((delta.T@w[:,:len(v)]).T*b_v)@x[n:n+maxN,:].reshape(maxN,input_N+1)
            learnw=delta@b.T
            #ê°ê°ì˜ ë°°ì—´ì— ì €ì¥     
            # v=v-learningrate*((sum_qw.T*b_v)@x[n,:].reshape(1,input_N+1))
                
            #ë°°ì—´ì˜ í‰ê·  ê°’ìœ¼ë¡œ ë°”ê¿”ì¤Œ
            learnw=learnw/maxN
            learnv=learnv/maxN
            #í•´ë‹¹ê°’ìœ¼ë¡œ ì—…ë°ì´íŠ¸
            v=v-learningrate*learnv
            w=w-learningrate*(learnw) 
           
        #í•œ ì—í­ì´ ì§€ë‚œ í›„ ì—…ë°ì´íŠ¸ ëœ vì™€ wë¥¼ ì´ìš©í•´ ì „ì²´ ë°ì´í„°ì— ëŒ€í•œ y_hatì„ êµ¬í•¨
        a_all = v @ x.T
        b_all = logistic(a_all)
        b_all = np.concatenate((b_all, one.T), axis=0)
        B_all = w @ b_all
        y_hat_all = logistic(B_all).T
        w_all.append(w)
        v_all.append(v)
        #mseê°’ê³¼ accê°’ì„ êµ¬í•¨
        mse.append(np.mean(np.sum((y_hat_all - one_hot_y) ** 2, axis=1)))
        acc.append(Accuracy_max(one_hot_y,y_hat_all,N))
        
    return mse,acc,w_all,v_all

#ì •í™•ë„ êµ¬í•˜ëŠ” í•¨ìˆ˜   
def Accuracy_max(one_hot_y,y_hat,N):#maxê¸°ì¤€
    #ì˜ˆì¸¡ê²°ê³¼ë¥¼ ì €ì¥ í•  í•¨ìˆ˜
    y_hat=y_hat.T
    y_hat_k=np.zeros([len(y_hat),y_hat.shape[1]])
    #y_hatê°’ ì¤‘í•œ ì—´ì— ëŒ€í•´ ìµœëŒ€ê°’ë§Œ 1ë¡œ ë³€í™˜ ë‚˜ë¨¸ì§€ëŠ” 0(ë„˜íŒŒì´ ë°°ì—´ íŠ¹ì„±:ê°’ì„ ë°”ê¿ˆ)
  
    for i in np.arange(0,N,1):
        y_hat_k[:,i]=(y_hat[:,i]==max(y_hat[:,i]))
    acc=np.mean(np.all(one_hot_y == y_hat_k.T, axis=1))
    return acc


#íŠ¹ì • ë°ì´í„° ì…‹ì„ train, test setìœ¼ë¡œ ë‚˜ëˆ ì£¼ëŠ” í•¨ìˆ˜
def Divide_data_set(new_xy_np,train,test): 
  
    
    #ê°€ì ¸ì˜¨ ë°ì´í„°ì˜ ë¹„ìœ¨ì„ êµ¬í•¨
    rate=len(new_xy_np)//10
    
    #ë°ì´í„° ë‚˜ëˆŒ êµ¬ê°„
    train_rate=train*rate
    test_rate=test*rate+train_rate
    
    #ì¸ë±ìŠ¤ ì…”í”Œ
    np.random.shuffle(new_xy_np)
    
    #ë°ì´í„°set ë‚˜ëˆ„ê¸°
    Training_set=new_xy_np[0:train_rate,:]
    Test_set=new_xy_np[train_rate:test_rate,:]
    data=pd.DataFrame(Training_set)

    
    Training_set=data.to_numpy(dtype="float64")#ë°ì´í„° í”„ë ˆì„ì„ ë„˜íŒŒì´ ë°°ì—´ë¡œ í˜•ë³€í™˜
    data=pd.DataFrame(Test_set)


    Test_set=data.to_numpy(dtype="float64")#ë°ì´í„° í”„ë ˆì„ì„ ë„˜íŒŒì´ ë°°ì—´ë¡œ í˜•ë³€í™˜
    
    
    return Training_set,Test_set


#ë‘ê°œì˜ ê³„ì¸µìœ¼ë¡œ ì´ë£¨ì–´ì§„ ì¸ê³µì‹ ê²½ë§ ì„¤ê³„ í•¨ìˆ˜
def ConfusionMatrix(data,data_x,data_y,Node,w,v,directory):
    #inputë°ì´í„° ì†ì„± ìˆ˜
    input_N=data_x.shape[1]
    one_hot_y=One_Hot_Encoding(data_y)
    one=np.ones([len(data_x),1])
    #ë”ë¯¸ë…¸ë“œë¥¼ ì¶”ê°€í•œ xë°ì´í„° ìƒì„±
    x=np.concatenate((data_x,one),axis=1)
    #Output í´ë˜ìŠ¤ ìˆ˜
    Q=len(np.unique(data_y))
    #íˆë“  layer ë…¸ë“œ ìˆ˜ 
    L=Node
    N=len(data_x)

    #íˆë“  layerì˜ aê°’ì„ êµ¬í•¨
    a=v@x.T
    #aê°’ì„ ë¡œì§€ìŠ¤í‹± í•¨ìˆ˜ì— í†µê³¼ì‹œì¼œ bë¥¼ êµ¬í•¨
    b=logistic(a)
    b=np.concatenate((b,one.T),axis=0)
    #ì¶œë ¥  layerì˜ Bê°’ì„ êµ¬í•¨
    B=w@b
    #Bê°’ì„ ë¡œì§€ìŠ¤í‹± í•¨ìˆ˜ì— í†µê³¼ì‹œì¼œ y_hatì„ êµ¬í•¨
    y_hat=logistic(B)

    y_hat_k=np.zeros([len(y_hat),y_hat.shape[1]])
    #y_hatê°’ ì¤‘í•œ ì—´ì— ëŒ€í•´ ìµœëŒ€ê°’ë§Œ 1ë¡œ ë³€í™˜ ë‚˜ë¨¸ì§€ëŠ” 0(ë„˜íŒŒì´ ë°°ì—´ íŠ¹ì„±:ê°’ì„ ë°”ê¿ˆ)
  
    for i in np.arange(0,N,1):
        y_hat_k[:,i]=(y_hat[:,i]==max(y_hat[:,i]))
    #í´ë˜ìŠ¤ ê°’ìœ¼ë¡œ ë¼ë²¨ë§    
    y_true=np.argmax(one_hot_y, axis=1)
    y_pred=np.argmax(y_hat_k.T, axis=1)
 
    
    wrong_idx = np.where(y_true != y_pred)[0]
    wrong_data = data_x[wrong_idx]
    
    #ë°ì´í„°ì…‹ì—ì„œ ì˜ˆì¸¡ì‹¤íŒ¨ ë°ì´í„° ì¸ë±ìŠ¤ êµ¬í•˜ê¸°
    data_index = []
    
    for wrong_sample in wrong_data:
        # dataì˜ ê° í–‰ê³¼ wrong_sampleì„ ë¹„êµ
        idx = np.where((data[:, :8] == wrong_sample[:8]).all(axis=1))[0]
        if len(idx) > 0:
            data_index.append(idx[0]) 
    data_index=np.array(data_index)
            
            
    k=3
    cluster,cluster_index=k_mean_clustering(k,wrong_data)
    for i, group in enumerate(cluster):
        print(f"[í´ëŸ¬ìŠ¤í„° {i}] ìƒ˜í”Œ ìˆ˜: {len(group)}")
        print(f"í´ëŸ¬ìŠ¤í„° {i} ì¸ë±ìŠ¤:", data_index[cluster_index[i]])  
        print("í‰ê·  íŠ¹ì§•ê°’:", np.round(np.mean(group, axis=0), 2))
        print() 
    file_list=sorted(os.listdir(directory),key=extract_two_numbers)
    visualize_cluster_sample(file_list, data_index, cluster_index,directory,max_per_cluster=5)
    
    
    #matrix ìƒì„±
    confusionmatrix = np.zeros((Q, Q), dtype=int)
    
    for i in range(len(y_true)):
        true_label = y_true[i]
        pred_label = y_pred[i]
        confusionmatrix[true_label][pred_label] += 1 
        
    correct = np.trace(confusionmatrix)   # ëŒ€ê°ì„  í•© (ì •ë¶„ë¥˜ ìˆ˜)
    total = np.sum(confusionmatrix)       # ì „ì²´ ìƒ˜í”Œ ìˆ˜
    accuracy = correct / total
    return confusionmatrix ,accuracy,cluster

def visualize_cluster_sample(images, wrong_idx, cluster_index, directory,max_per_cluster=5):
    """
    ê° í´ëŸ¬ìŠ¤í„°ì—ì„œ ëŒ€í‘œ ì´ë¯¸ì§€ë“¤ì„ ì‹œê°í™”

    Parameters:
        images           : ì „ì²´ ì´ë¯¸ì§€ ë°°ì—´ (index ê¸°ì¤€)
        wrong_idx        : ì˜¤ë¶„ë¥˜ëœ ìƒ˜í”Œì˜ ì „ì²´ ì¸ë±ìŠ¤ (data_x ê¸°ì¤€ â†’ images ì¸ë±ìŠ¤ì™€ ì—°ê²°)
        cluster_indices  : k_mean_clustnering_with_index()ê°€ ë°˜í™˜í•œ í´ëŸ¬ìŠ¤í„°ë³„ ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸
        max_per_cluster  : í´ëŸ¬ìŠ¤í„°ë‹¹ ì‹œê°í™”í•  ìƒ˜í”Œ ìˆ˜ (ê¸°ë³¸ê°’: 5)
    """

    k = len(cluster_index)
    for j in range(k):
        print(f"\nğŸ“¦ í´ëŸ¬ìŠ¤í„° {j} (ìƒ˜í”Œ ìˆ˜: {len(cluster_index[j])})")
        fig, axes = plt.subplots(1, min(max_per_cluster, len(cluster_index[j])), figsize=(15, 3))
        if len(cluster_index[j]) == 1:
            axes = [axes]
        for i, ax in enumerate(axes):
            if i >= len(cluster_index[j]):
                break
            img_index = wrong_idx[cluster_index[j][i]]  # ì‹¤ì œ ì´ë¯¸ì§€ ì¸ë±ìŠ¤
            name=images[img_index]
            path=os.path.join(directory,name)
            img_GRB = cv2.imdecode(np.fromfile(path, dtype=np.uint8), cv2.IMREAD_COLOR)
               
            img_RGB=cv2.cvtColor(img_GRB,cv2.COLOR_BGR2RGB)
            ax.imshow(img_RGB)
            ax.set_title(f"Index: {img_index}")
            ax.axis("off")
        plt.suptitle(f"Cluster {j} ëŒ€í‘œ ì´ë¯¸ì§€", fontsize=14)
        plt.show()
        
def k_mean_clustering(k,data):

    n=len(data)
    init_index=np.random.choice(n, size=k, replace=False)
    init_mid=data[init_index]
    pre_mid=np.zeros([k,data.shape[1]])
    new_mid=np.zeros([k,data.shape[1]])
    pre_mid=init_mid
    while(1):  
        
       
        L2_norm=[]
        for j in np.arange(0,k,1):
            L2=[]
            for i in np.arange(0,n,1):
                L2.append(np.sqrt(np.sum((data[i] - pre_mid[j])**2)))
            L2_norm.append(L2)
        L2_norm=np.array(L2_norm)
        L2_norm=L2_norm.T
    
        cluster=np.argmin(L2_norm, axis=1)
       
        for j in np.arange(k):
            cluster_point=data[cluster==j]
            if(len(cluster_point)>0):
                new_mid[j]=np.mean(cluster_point,axis=0)
                
    
        if(np.array_equal(new_mid, pre_mid)):
            break
        else:
            pre_mid=new_mid
    cluster_array=[]     
    for j in np.arange(k):
        cluster_array.append(data[cluster==j])
    cluster_index=[]
    for j in range(k):
        cluster_index.append(np.where(cluster==j)[0])
    return cluster_array,cluster_index


#ë‘ê°œì˜ ê³„ì¸µìœ¼ë¡œ ì´ë£¨ì–´ì§„ ì¸ê³µì‹ ê²½ë§ ì„¤ê³„ í•¨ìˆ˜
def test_accuracy(data_x,data_y,Node,w,v):
    #inputë°ì´í„° ì†ì„± ìˆ˜
    input_N=data_x.shape[1]
    one_hot_y=One_Hot_Encoding(data_y)
    one=np.ones([len(data_x),1])
    #ë”ë¯¸ë…¸ë“œë¥¼ ì¶”ê°€í•œ xë°ì´í„° ìƒì„±
    x=np.concatenate((data_x,one),axis=1)
    #Output í´ë˜ìŠ¤ ìˆ˜
    Q=len(np.unique(data_y))
    #íˆë“  layer ë…¸ë“œ ìˆ˜ 
    L=Node
    N=len(data_x)
    mse=[]
    acc=[]
    for k in np.arange(0,len(w),1):
        w_t=np.array(w[k])
        v_t=np.array(v[k])
    #íˆë“  layerì˜ aê°’ì„ êµ¬í•¨
        a=v_t@x.T
        #aê°’ì„ ë¡œì§€ìŠ¤í‹± í•¨ìˆ˜ì— í†µê³¼ì‹œì¼œ bë¥¼ êµ¬í•¨
        b=logistic(a)
        b=np.concatenate((b,one.T),axis=0)
        #ì¶œë ¥  layerì˜ Bê°’ì„ êµ¬í•¨
        B=w_t@b
        #Bê°’ì„ ë¡œì§€ìŠ¤í‹± í•¨ìˆ˜ì— í†µê³¼ì‹œì¼œ y_hatì„ êµ¬í•¨
        y_hat=logistic(B)
        mse.append(np.mean(np.sum((y_hat.T - one_hot_y) ** 2, axis=1)))
        acc.append(Accuracy_max(one_hot_y,y_hat.T,N))
           
 
    return mse,acc


'''
R	G	B	ìƒ‰ìƒ ì„¤ëª…
0	0	0	ê²€ì • (black)
255	255	255	í°ìƒ‰ (white)
255	0	0	ë¹¨ê°• (pure red)
0	255	0	ì´ˆë¡ (pure green)
0	0	255	íŒŒë‘ (pure blue)
255	255	0	ë…¸ë‘ (red + green)
0	255	255	ì²­ë¡ (green + blue)
255	0	255	ìí™/í•‘í¬ (red + blue)
'''
def extract_two_numbers(filename):
   # ì˜ˆ: '0_10.jpg' â†’ (0, 10)
   name = os.path.splitext(filename)[0]  # í™•ì¥ì ì œê±°: '0_10'
   parts = name.split('_')
   return (int(parts[0]), int(parts[1]))

import os
import cv2
import numpy as np
def select_features(directory):
  
    file_list = sorted(os.listdir(directory), key=extract_two_numbers)
    feature_1_list=[]
    feature_2_list=[]
    feature_3_list=[]
    feature_4_list=[]
    feature_5_list=[]
    feature_6_list=[]
    feature_7_list=[]
    feature_8_list=[]
    feature_9_list=[]
    feature_10_list=[]
    labels=[]
    for name in file_list:
        if(name=="7_0.jpg"):
            print("ì§€ê¸ˆ")
        path=os.path.join(directory,name)
        labels.append(int(name.split('_',1)[0]))
        img_GRB = cv2.imdecode(np.fromfile(path, dtype=np.uint8), cv2.IMREAD_COLOR)
           
        img_RGB=cv2.cvtColor(img_GRB,cv2.COLOR_BGR2RGB)
       
        
        #íŠ¹ì§• 1 ë¸”ë ‰ë² ë¦¬
        feature_1=is_blackberry(img_RGB)
        
        #íŠ¹ì§• 2 ë°°
        if(feature_1):
            feature_2=0
        else:
            feature_2=is_bae(img_RGB)
            
         #íŠ¹ì§• 3 ì²­ì‚¬ê³¼, ìˆ˜ë°•  
        if(feature_1): 
            feature_3=0
        else:
            feature_3=is_green(img_RGB)
            
        #ì²­ì‚¬ê³¼,ìˆ˜ë°• êµ¬ë¶„  
        if(feature_1):
            feature_4=0
        elif(feature_3):
            feature_4=left_right(img_RGB)
        else:
            feature_4=0
         
         #ì˜¤ì´ êµ¬ë¶„ 
        if(feature_1 or  feature_2 or feature_3 or feature_4):  
            feature_5=0
        else:
             feature_5=is_oi(img_RGB)
                
             
           #ë°”ë‚˜ë‚˜     
        if(feature_1 or feature_3 or feature_4 or feature_5):  
            feature_6=0
        else:
             feature_6=is_white(img_RGB)
      
        
        
            
        if(feature_1 or feature_3 or feature_4 or feature_5):  
            feature_7=0
        else:
            if feature_6>=0.45:
                feature_7=0
            else:
                feature_7=right_left(img_RGB)
                
        
        if(feature_1 or feature_2 or feature_3 or feature_4 or feature_5 or feature_6 or feature_7):  
            feature_8=0
        else:
             feature_8=np.mean(img_RGB)/255
             
             
        if(feature_1 or feature_2 or feature_3 or feature_4 or feature_5 or feature_6 or feature_7):  
            feature_9=0
        else:
            feature_9=np.mean(img_RGB[:,:,0])/255
             
             
        
        
        
        feature_1_list.append(feature_1)
        feature_2_list.append(feature_2)
        feature_3_list.append(feature_3)
        feature_4_list.append(feature_4)
        feature_5_list.append(feature_5)
        feature_6_list.append(feature_6)
        feature_7_list.append(feature_7)
        feature_8_list.append(feature_8)
        feature_9_list.append(feature_9)
        # feature_10_list.append(feature_10)
        
    feature_1_list=np.array(feature_1_list)
    feature_2_list=np.array(feature_2_list)
    feature_3_list=np.array(feature_3_list)
    feature_4_list=np.array(feature_4_list)
    feature_5_list=np.array(feature_5_list)
    feature_6_list=np.array(feature_6_list)
    feature_7_list=np.array(feature_7_list)
    feature_8_list=np.array(feature_8_list)
    feature_9_list=np.array(feature_9_list)
    # feature_10_list=np.array(feature_10_list)
    features=np.column_stack([feature_1_list, feature_2_list,
                              feature_3_list,feature_4_list,feature_5_list,feature_6_list
                              ,feature_7_list
                                , feature_8_list
                                 ,feature_9_list
                               #  ,feature_10_list
                              ])
    
    
    return features,labels
    
def is_blackberry(img):
   # ë§ˆìŠ¤í¬ë¡œ ê²€ì€ í”½ì…€ ë¹„ìœ¨ ê³„ì‚°

    r, g, b = img[ :, :,0], img[ :, :,1], img[ :, :,2]
    black_mask = (r < 50) & (g <50) & (b < 50)
    green_mask = (g > 130) & (g > r + 30) & (g > b + 30) 
    green_ratio=np.sum(green_mask) / (img.shape[0] * img.shape[1])
    black_ratio=np.sum(black_mask) / (img.shape[0] * img.shape[1])
    if(black_ratio>0.1 and green_ratio<0.7):
        return 1
    else:
        return 0
def is_bae(img):

    r, g, b = img[ :, :,0], img[ :, :,1], img[ :, :,2]
    bae_mask= (
    (r > 175) &       # ë” ê°•í•œ ë¹¨ê°•
    (g > 140) &       # ë” ê°•í•œ ì´ˆë¡
    (b < 150)      # ë” ë‚®ì€ íŒŒë‘ 
)
   
    bae_ratio = np.sum(bae_mask) / (img.shape[0] * img.shape[1])
  

    # ìœ ì¼ íŠ¹ì„± ì¡°ê±´:
    if(bae_ratio > 0.2 ):
        return bae_ratio
    else:
        return 0
 
 
def is_oi(img):

    r, g, b = img[ :, :,0], img[ :, :,1], img[:, :,2]
    white_mask= (r>240)&(b>240)&(g>240)
    green_mask=(g > r + 20) & (g > b + 20)&(b<250)&(r<250)
    
    white_ratio = np.sum(white_mask) / (img.shape[0] * img.shape[1])
    green_ratio = np.sum(green_mask) / (img.shape[0] * img.shape[1]) 
    # ìœ ì¼ íŠ¹ì„± ì¡°ê±´:
    if(white_ratio > 0.5 and green_ratio >0.2 ):
        return 1
    else:
        return 0
    
def is_white(img):

    r, g, b = img[ :, :,0], img[ :, :,1], img[:, :,2]
    white_mask= (r>240)&(b>100)&(g>200)
    fruit=~white_mask
    yellow_mask=(r>120)&(b<100)&(g>120)    
    white_ratio = np.sum(white_mask) / (img.shape[0] * img.shape[1])
   
    # green_ratio = np.sum(green_mask) / (img.shape[0] * img.shape[1]) 
    # ìœ ì¼ íŠ¹ì„± ì¡°ê±´:
        
    if(white_ratio > 0.4):
        return white_ratio
    else:
        return 0
 
def is_green(img):
    r, g, b = img[ :, :,0], img[ :, :,1], img[ :, :,2]
    green_mask=(g > r + 20) & (g > b + 20) & (g >100)&(b<250)&(r<250)
    green_ratio = np.sum(green_mask) / (img.shape[0] * img.shape[1]) 
    # ìœ ì¼ íŠ¹ì„± ì¡°ê±´:
    if(green_ratio > 0.45 ):
        return 1
    else:
        return 0
    
    
    
def left_right(img):   
    g= img[ :, :,1]
    g_filtered = g.copy() 
    g_filtered = g.astype(float) 
    g_filtered[g_filtered >= 250] =np.nan  # í° ë°°ê²½ â†’ NaNìœ¼ë¡œ ì œì™¸
    
    # ì¢Œìš° í‰ê·  (NaN ì œì™¸í•˜ê³  í‰ê· )
    bright_left = np.nanmean(g_filtered[:, :128])
    bright_right = np.nanmean(g_filtered[:, 128:])
    
    # ì¢Œìš° ë°ê¸° ì°¨ì´ ë¹„ìœ¨
    bright = (bright_left - bright_right) / (bright_left + bright_right)
    
    if( bright > 0.09 ):
        return 1
    else:
        return 0
    return bright

def right_left(img):   
    r= img[ :, :,0]
    r_filtered = r.copy() 
    r_filtered = r.astype(float) 
    r_filtered[r_filtered >= 250] =np.nan  # í° ë°°ê²½ â†’ NaNìœ¼ë¡œ ì œì™¸
    
    # ì¢Œìš° í‰ê·  (NaN ì œì™¸í•˜ê³  í‰ê· )
    bright_left = np.nanmean(r_filtered[:, :128])
    bright_right = np.nanmean(r_filtered[:, 128:])
    
    # ì¢Œìš° ë°ê¸° ì°¨ì´ ë¹„ìœ¨
    bright = (bright_right - bright_left) / (bright_left + bright_right)
    
    if( bright > 0 ):
        return 1
    else:
        return 0
    return bright


def is_apple(img):
    r, g, b = img[ :, :,0], img[ :, :,1], img[ :, :,2]
    apple_mask=(r > 140) & (g < 130) & (b < 100) & (r > g + 20) & (r > b + 20)
    apple_ratio = np.sum(apple_mask) / (img.shape[0] * img.shape[1]) 
    # ìœ ì¼ íŠ¹ì„± ì¡°ê±´:
    if(apple_ratio > 0.25):
        return 1
    else:
        return 0
    
    
    


#íˆë“  ë…¸ë“œ ìˆ˜
Node=50
#ë°ì´í„° ì…‹ ë¶„í•  ë¹„ ì„¤ì •
train=7
test=3

#batch ê°’ ì„¤ì •
batch=16
epoch=300
init_1=1
directory="C:\\Users\\ì´ìƒí›ˆ\\OneDrive - í•œêµ­ê³µí•™ëŒ€í•™êµ\\ë°”íƒ• í™”ë©´\\ì¸ì„¤\\data_set"
X,y=select_features(directory)
y=np.array(y)
y=y.reshape(7000,1)


data =np.hstack((X, y)) 

confusionmatrix=[]

#ì „ì²´ì ì¸ ì„±ëŠ¥ í‰ê· ì„ êµ¬í•˜ê¸° ìœ„í•´ ì‚¬ìš©
n=1
for i in np.arange(n):
    #ë°ì´í„° ë¶„í• 
    Train,Test=Divide_data_set(data.copy(),train,test)
    train_x=Train[:,:data.shape[1]-1]
    train_y=Train[:,data.shape[1]-1:data.shape[1]].reshape(len(Train),1)
    test_x=Test[:,:data.shape[1]-1]
    test_y=Test[:,data.shape[1]-1:data.shape[1]].reshape(len(Test),1)
    
    mse, acc, w, v = Error_Back_Propagation1(Train, Node,batch,epoch)
    
    
    #confusionmatrix ë„ì¶œ
    conma,acc_t,cluster=ConfusionMatrix(data,test_x, test_y, Node, w[-1], v[-1],directory)
    confusionmatrix.append(conma)
    
confusionmatrixnum=sum(confusionmatrix)/n



# #test_setì— ëŒ€í•œ ê·¸ë˜í”„ë¥¼ ë„ì¶œ í•˜ê¸° ìœ„í•´ ì‚¬ìš©
# Train,Test=Divide_data_set(data.copy(),train,test)
# train_x=Train[:,:data.shape[1]-1]
# train_y=Train[:,data.shape[1]-1:data.shape[1]].reshape(len(Train),1)
# test_x=Test[:,:data.shape[1]-1]
# test_y=Test[:,data.shape[1]-1:data.shape[1]].reshape(len(Test),1)
# Node=[10,50,100]
# confusionmatrix=[]
# for n in Node:
#     mse, acc, w, v = Error_Back_Propagation1(Train, n,batch,epoch)
#     #confusionmatrix ë„ì¶œ
#     confusionmatrix.append(ConfusionMatrix(test_x, test_y, n, w[-1], v[-1]))
#     mse_t,acc_t= test_accuracy(test_x, test_y, Node, w, v)
#     if(n==10):
#         mse10,acc10=mse_t,acc_t
#     elif(n==50):
#         mse50,acc50=mse_t,acc_t
#     else:
#         mse100,acc100=mse_t,acc_t
     
        
        
        
